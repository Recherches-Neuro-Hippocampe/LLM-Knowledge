{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Knowledge\n",
    "\n",
    "This is a handbook with all necessary knowledge about chatgpt and other LLMs for GeneRXN. \n",
    "\n",
    "## Chatgpt related pratical tips\n",
    "\n",
    "### 1. Token limits\n",
    "\n",
    "#### 1.1 What is a token for gpt and how do we count in real use\n",
    "\n",
    "**What is a token** : Token is a concept in natural language which we use to break down a sentence into meta units, including words, punctuation marks... Depending on the tokenizing techniques used, a word can also be further broken down into prefix and postfix etc. According to Openai the approximation for tokens is: \n",
    "* 1 token ~= 4 chars in English\n",
    "\n",
    "* 1 token ~= Â¾ words\n",
    "\n",
    "* 100 tokens ~= 75 words\n",
    "\n",
    "Or \n",
    "\n",
    "* 1-2 sentence ~= 30 tokens\n",
    "\n",
    "* 1 paragraph ~= 100 tokens\n",
    "\n",
    "* 1,500 words ~= 2048 tokens \n",
    "\n",
    "#### 1.2 Token limitaions \n",
    "\n",
    "* **The limitation is shared between prompt and completion**. Prompt is your input and the completion is the answer from chatgpt. If you have a token limitation up to 4096 and your prompt is 4000 tokens, your completion can be 97 tokens at most. \n",
    "\n",
    "* Token limitations for different models\n",
    "\n",
    "|     Model     | Maximum Tokens |\n",
    "|:-------------:|:--------------:|\n",
    "| gpt-3.5-turbo |  4,096 tokens  |\n",
    "|     gpt-4     |  8,192 tokens  |\n",
    "|   gpt-4-32k   |  32,768 tokens |\n",
    "\n",
    "#### 1.3 Token pricing\n",
    "\n",
    "* This is obtained from [Openai website](https://openai.com/pricing)\n",
    "\n",
    "|     Model     |       Prompt       |     Completion     |\n",
    "|:-------------:|:------------------:|:------------------:|\n",
    "| gpt-3.5-turbo | $0.002 / 1K tokens | $0.002 / 1K tokens |\n",
    "|     gpt-4     |  $0.03 / 1K tokens |  $0.06 / 1K tokens |\n",
    "|   gpt-4-32k   |  $0.06 / 1K tokens |  $0.12 / 1K tokens |\n",
    "\n",
    "\n",
    "### 2. Memory retenant (chat history preservation)\n",
    "\n",
    "Think in this way, a model is basically a huge chunk of math calculations, therefore **there is no memory for the model**. Chatgpt and other chat bots use certain techniques to make chatgpt feel like it remember the previous chat history. One way to achieve that is by **feeding the previous messages to GPT**. To be more detailed, every single time we send a request to GPT, we have to firstly collect the chat log, attach the current prompt to it and then feed it to the model (Which is indeed inefficient when the conversation goes on and on).\n",
    "\n",
    "An example is shown below, which is from the repo [here](https://github.com/atomic14/command_line_chatgpt/blob/main/main.py).\n",
    "**The general logic is feeding the model the general context first, then every single time the model is fed by the combination of the chat log and current prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(instructions, previous_questions_and_answers, new_question):\n",
    "    \"\"\"Get a response from ChatCompletion\n",
    "\n",
    "    Args:\n",
    "        instructions: The instructions for the chat bot - this determines how it will behave\n",
    "        previous_questions_and_answers: Chat history\n",
    "        new_question: The new question to ask the bot\n",
    "\n",
    "    Returns:\n",
    "        The response text\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": instructions },\n",
    "    ]\n",
    "    # add the previous questions and answers\n",
    "    for question, answer in previous_questions_and_answers[-MAX_CONTEXT_QUESTIONS:]:\n",
    "        messages.append({ \"role\": \"user\", \"content\": question })\n",
    "        messages.append({ \"role\": \"assistant\", \"content\": answer })\n",
    "    # add the new question\n",
    "    messages.append({ \"role\": \"user\", \"content\": new_question })\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        top_p=1,\n",
    "        frequency_penalty=FREQUENCY_PENALTY,\n",
    "        presence_penalty=PRESENCE_PENALTY,\n",
    "    )\n",
    "return completion.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Work around token limitations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
